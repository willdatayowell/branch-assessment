{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "366377c5",
   "metadata": {},
   "source": [
    "# Branch Assessment\n",
    "Project created and best viewed in [Jupyter Notebooks](https://www.anaconda.com/products/individual)\n",
    "\n",
    "\n",
    "## Deliverables\n",
    "- Deliverable 1 - [Entity Relationship Diagram](https://dbdiagram.io/d/61fd6ea885022f4ee53ec996) or the PDF inside this project (branch-assessment-erd.pdf)\n",
    "- Deliverable 2 - You're looking at it\n",
    "- Deliverable 3 -\n",
    "> Q: Now imagine this is a production ETL process. How would you design it? What tests would you put in place?\n",
    "> \n",
    "> A:\n",
    ">\n",
    "> I would use business requirements to drive data quality tests and data transformation requirements before a record would get written to the source database.\n",
    ">\n",
    ">I would orchestrate the ETL workflow to be idempotent to ensure all CRUD functions were able to be performed successfully before anything gets written/committed to the destination warehouse. This prevents problems with failures happening in the middle of the process. If anything failed, alerts would be in place to let the responsible parties aware of the failure so swift intervention could take place. Since the process is idempotent, once the issue is fixed, the workflow can be rerun without fear of corrupting data. Included in this project is a screenshot of an idempotent workflow using an ETL tool I've used in the past (etl-screenshot.png). The example is for SCD Type 2 but the concept is the same. \n",
    "\n",
    "## My assumptions for this project\n",
    "- There might be a data governance need to break up the data into separate tables. With no imposed restrictions however, I would prefer to store the JSON response as a whole and implementing a schema-on-read solution instead of a legacy RDBMS solution\n",
    "- Dimensions were ok to be SCD Type 1 (overwritten when updated, no history)\n",
    "- Storing the password and identity/ssn info in plain text was bait, so I only stored the hashed versions of those attributes\n",
    "- The pros/cons of a flat schema outweighed the pros/cons of a star schema.\n",
    "\n",
    "> #### Flat Schema (distributed UUIDs for every table)\n",
    "> Pros : Simplicity of CRUD functions. Performance increase for reporting queries. Flexibility of walking the tables from any starting point.\n",
    "> Cons : UUID duplication / denormalization. Slight performance hit on CRUD funtions.\n",
    ">\n",
    "> #### Star Schema\n",
    "> Pros : Performance increase of CRUD functions. Data normalization.\n",
    "> Cons : More complex ETL configuration (order of operations). Extraneous fact table for taxonomy only. Slight performance hit for reporting queries. Fact table needs to be your anchor for all queries.\n",
    "\n",
    "[Learn more about me](https://shorturl.at/ruRV8)\n",
    "\n",
    ":)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9adf8f57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import json\n",
    "import os\n",
    "import hashlib\n",
    "from urllib.request import urlopen\n",
    "\n",
    "source_url = 'https://randomuser.me/api/?results=500'\n",
    "source_response = urlopen(source_url)\n",
    "source_data = json.loads(source_response.read())\n",
    "\n",
    "df = pandas.json_normalize(source_data['results'])\n",
    "\n",
    "#\n",
    "# Create new column in dataframe for hashed identity/ssn\n",
    "#\n",
    "df['hash'] = numpy.where(df['id.value'].isnull(), '', df['id.value'].astype(str).apply(lambda x: hashlib.sha256(x.encode()).hexdigest()))\n",
    "\n",
    "\n",
    "#\n",
    "# Column naming\n",
    "#\n",
    "df.columns\n",
    "df.rename(columns={\n",
    "    'login.uuid': 'uuid',\n",
    "    'login.username': 'username',\n",
    "    'login.salt': 'salt',\n",
    "    'login.sha256': 'sha256',\n",
    "    'location.street.number': 'street_number',\n",
    "    'location.street.name': 'street_name',\n",
    "    'location.city': 'city',\n",
    "    'location.state': 'state',\n",
    "    'location.country': 'country',\n",
    "    'location.postcode': 'postal_code',\n",
    "    'location.coordinates.latitude': 'lat',\n",
    "    'location.coordinates.longitude': 'long',\n",
    "    'location.timezone.offset': 'utc_offset',\n",
    "    'location.timezone.description': 'utc_description',\n",
    "    'name.title': 'name_salutation',\n",
    "    'name.first': 'name_first',\n",
    "    'name.last': 'name_last',\n",
    "    'dob.date': 'dob',\n",
    "    'picture.thumbnail': 'picture_thumbnail',\n",
    "    'picture.medium': 'picture_medium',\n",
    "    'picture.large': 'picture_large',\n",
    "    'registered.date': 'date_registered',\n",
    "    'id.name': 'type'\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "#\n",
    "# Create dataframes for each table \n",
    "#\n",
    "\n",
    "auth_table = df[[\n",
    "    'uuid',\n",
    "    'username',\n",
    "    'salt',\n",
    "    'sha256'\n",
    "]]\n",
    "\n",
    "locations_table = df[[\n",
    "    'uuid',\n",
    "    'street_number',\n",
    "    'street_name',\n",
    "    'city',\n",
    "    'state',\n",
    "    'country',\n",
    "    'postal_code',\n",
    "    'lat',\n",
    "    'long',\n",
    "    'utc_offset',\n",
    "    'utc_description'\n",
    "]]\n",
    "\n",
    "user_details_table = df[[\n",
    "    'uuid',\n",
    "    'gender',\n",
    "    'name_salutation',\n",
    "    'name_first',\n",
    "    'name_last',\n",
    "    'email',\n",
    "    'dob',\n",
    "    'phone',\n",
    "    'cell',\n",
    "    'picture_thumbnail',\n",
    "    'picture_medium',\n",
    "    'picture_large',\n",
    "    'date_registered'\n",
    "]]\n",
    "\n",
    "identities_table = df[[\n",
    "    'uuid',\n",
    "    'type',\n",
    "    'hash'\n",
    "]]\n",
    "\n",
    "auth_table.to_csv('csvs/auth_table.csv')\n",
    "locations_table.to_csv('csvs/locations_table.csv')\n",
    "user_details_table.to_csv('csvs/user_details_table.csv')\n",
    "identities_table.to_csv('csvs/identities_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c40d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
